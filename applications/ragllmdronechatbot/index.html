<!DOCTYPE html>
<!-- RAG Drone Chatbot application -->
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Applications</title>
    <link rel="stylesheet" href="../../style.css">
    <script src="../../script.js" defer></script>
    <script src="../../applications/grabJson.js" defer></script>
</head>
<body>
    <nav class="header-bar">
        <div class="nav-left">
            <a href="../../" class="nav-link">Home</a>
            <a href="../" class="nav-link">Applications</a>
        </div>
        <div class="nav-right">
            <button id="theme-toggle" aria-label="Activate Dark Mode">ðŸŒ“</button>
            <a href="https://github.com/JoshhChoong" class="nav-icon" aria-label="GitHub" target="_blank" rel="noopener noreferrer">
                <svg class="nav-icon-svg" viewBox="0 0 24 24" aria-hidden="true"><path fill="currentColor" d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
            </a>
            <a href="https://www.linkedin.com/in/joshua-choong-36677a209" class="nav-icon" aria-label="LinkedIn" target="_blank" rel="noopener noreferrer">
                <svg class="nav-icon-svg" viewBox="0 0 24 24" aria-hidden="true"><path fill="currentColor" d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
            </a>
        </div>
    </nav>
    <div id="content">
        <h1 class="center-header">RAG LLM Drone Chatbot</h1>
        <div>
            <div class="blog-post-meta">
                <span>Joshua Choong</span>
                <span>February 21</span>
            </div>
            <div class="blog-post-video">
                <iframe src="https://www.youtube.com/embed/1lQo_m-fVo8" title="RAG Drone Chatbot - YouTube" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                <p><a href="https://www.youtube.com/watch?v=1lQo_m-fVo8" target="_blank" rel="noopener noreferrer">Watch on YouTube</a></p>
            </div>
        </div>
        <h2>Introduction:</h2>
        <p>
            The overall goal with this project was to build a Retrieval-Augmented Generation 
            (RAG) system focused on Canadian drone rules. This project was designed to create
            easily accesible infromation for the public to understand, as I found that geographical
            jursticiional differences in drone rules were not well understood. I wanted to
            to get familiar with a full RAG pipeline: ingesting source documents, chunking and
             embedding them, storing vectors in a database, and generating answers with cited 
             sources.
        </p>
        <p>
            I implemented a pipeline that ingests JSON and HTML, chunks and cleans the text, embeds it with sentence-transformers (with an optional Gemini API fallback), and stores vectors in ChromaDB. At query time, the system retrieves relevant chunks and uses an LLM via OpenRouter or Google Gemini to generate an answer with source citations. Through building this, I learned that data hygiene stripping boilerplate, preserving metadata like title and URL, and using overlapping chunks makes a real difference in retrieval quality and traceability. The repo also includes an AutoImmunePipeline concept to detect and mitigate data poisoning and jurisdiction-mismatched content, which felt like good practice for a public-policy RAG use case.
        </p>
        <p>
            I learned a lot about RAG fundamentals and how to keep a system resilient under rate limits and API quotas by using multi-layer fallbacks (local vs API embeddings, LLM model fallbacks, and quota-aware retries). Steps for future development would include expanding the rule corpus, tuning chunk sizes and overlap, and experimenting with different embedding models.
        </p>

        <h2>Contents:</h2>
        <ul>
            <li><a href="#ragSection1">1. Pipeline &amp; Purpose</a></li>
            <li><a href="#ragSection2">2. Technologies Used</a></li>
            <li><a href="#ragSection3">3. Learning Takeaways</a></li>
        </ul>

        <h2 id="ragSection1">1. Pipeline &amp; Purpose</h2>
        <p>
            I started by defining the main flow: ingest JSON/HTML source data, chunk and clean the text, embed the chunks, and store them in ChromaDB. At query time, the system retrieves the most relevant chunks, then constructs a context-aware prompt and calls an LLM to generate an answer with cited sources.
        </p>
        <p>
            The key modules are <code>app.py</code> (Flask runner), <code>ingestion.py</code>, <code>embeddings.py</code>, <code>retrieval.py</code>, and <code>generation.py</code>. Configuration and API keys live in <code>.env</code> (e.g. <code>OPENROUTER_API_KEY</code>, <code>GEMINI_API_KEY</code>), and I used <code>python-dotenv</code> to load them. Persisting Chroma, closing clients properly, and managing <code>.env</code> are essential for reproducible local runs and deployments.
        </p>

        <h2 id="ragSection2">2. Technologies Used</h2>
        <p>
            I used Python with Flask for the app runner and a local venv; dependencies are in <code>requirements.txt</code>. For the vector store I went with ChromaDB (persistent client, with duckdb+parquet fallback and local SQLite under <code>chroma_db</code>). Embeddings are from local sentence-transformers (paraphrase-MiniLM) with an optional Google/Gemini REST fallback. For the LLM I used OpenRouter (OpenAI-compatible) and direct Google Gemini REST calls, with model and fallback logic so the system stays usable under rate or credit limits.
        </p>
        <p>
            Chunking is done with tiktoken and sentence-based chunking plus overlap heuristics so that retrieval gets coherent segments and we donâ€™t lose context at boundaries.
        </p>

        <h2 id="ragSection3">3. Learning Takeaways</h2>
        <p>
            A practical RAG pipeline text cleaning, sensible chunking, embeddings, vector search, and context-aware prompt construction really does yield focused, source-backed answers. Multi-layer fallbacks (local vs API embeddings, LLM model fallbacks, quota-aware retries) keep the system usable when APIs throttle or run out of credits.
        </p>
        <p>
            It is worth noting that stripping boilerplate, preserving metadata (title/URL), and using overlapping chunks improved retrieval quality and made it easier to trace answers back to the source. The repoâ€™s AutoImmunePipeline concept for detecting data poisoning and jurisdiction-mismatched content is a good practice for any public-policy RAG application.
        </p>
    </div>
</body>
</html>
